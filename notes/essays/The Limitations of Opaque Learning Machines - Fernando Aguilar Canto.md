# Sobre "The Limitations of Opaque Learning Machines" (Pearl, 2019)

### _Fernando Aguilar Canto_

¿Pueden las máquinas superar a las capacidades cognitivas humanas? Esta pregunta es cuando menos, extrema, y no se pretende responder o dar un posicionamiento al respecto en la extensión de un ensayo. Judea Pearl tampoco responde dicha pregunta en su reporte técnico (2019). El avance vertiginoso de la Inteligencia Artificial (IA) contemporánea, en buena medida debida al surgimiento del Deep Learning y el aumento del poder de cómputo, parece poder extrapolarse hacia la llamada "singularidad tecnológica" discutiva por varios autores (véase por ejemplo Shanahan, 2015). No obstante, la tesis fundamental de Pearl consiste en que el enfoque predominante que rige al Deep Learning y al Aprendizaje Automático en general es básicamente incapaz de lograr superar los niveles cognitivos humanos, por el simple hecho de carecer de capacidades como el razonamiento causal.

En este sentido, los modelos de Aprendizaje Automático predominantes funcionan en el mundo estadístico, de manera "ciega", o más bien por medio de un aprendizaje meramente pasivo. Para Pearl, esta forma de entrenar a sistemas con Inteligencia Artificial nunca podrá alcanzar a la inteligencia humana, a pesar de que puedan superar a los humanos en diversos ámbitos específicos (mas nunca generales). De acuerdo con este esquema, las preguntas que sirven de base para el desarrollo científico, en gran medida son contrafactuales o intervencionales y a ese nivel no puede llegar un enfoque basado exclusivamente en datos. Por ende, la IA-Fuerte resulta inalcanzable bajo este esquema.

¿Esto significa que la IA-Fuerte es completamente inalcanzable? Los argumentos de Pearl no inducen que esto sea imposible, simplemente se orientan a decir cuáles son los límites de los métodos exclusivamente basados en datos. Los agentes pueden disponer de una cantidad extremadamente grande de datos, pero si sólo los observan, no podrían responder determinadas preguntas que los humanos, al parecer, sí pueden. 

El planteamiento de Pearl remite a un viejo problema filosófico del innatismo contra el empirismo: ¿es la mente una tabla rasa ("tabula rasa") o dispone de cierta información preliminar? En el empirismo de Locke y los aristotélicos, la mente nace como una hoja en blanco, carente de información sobre el mundo, pero que se va llenando de información a través de la experiencia (Duschinsky, 2012). Aunque el innatismo parece ser una perspectiva superada por los empiristas, para autores relativamente recientes como Chomsky (2014), las estructuras innatas son necesarias para el habla. 

Sin embargo, considero que este problema debe buscar solución en las neurociencias. ¿Hasta qué punto las redes neuronales biológicas son un sistema que se organiza con los datos? Algunos aspectos, como la arquitectura de la red, parecen estar moldeados por la evolución, mientras que otros, como los pesos de las redes neuronales, en gran medida se ajustan con los datos que se van recibiendo (plasticidad).

Una versión contemporánea del problema del innatismo-empirismo corresponde a un dominio específico de las redes neuronales biológicas, que es referente al Aprendizaje por Refuerzo. Al leer a Pearl, da un poco la impresión de que se refiere al Aprendizaje de Máquina únicamente como al Supervisado y No Supervisado, sin tomar en cuenta que el Aprendizaje por Refuerzo requiere de una toma de acciones, por lo que de alguna forma quizá puedan responder las preguntas intervencionales. 

En la teoría del Aprendizaje por Refuerzo, existe una discusión muy amplia sobre el uso de métodos libres de modelos (_model-free_) y los métodos basados en modelos (_model-based_). Los primeros corresponden mejor al ejemplo de la tabla rasa, ya que no asumen supuestos sobre el entorno. Para Pearl, es necesario hacer estos supuestos, y con ello se puede alcanzar a responder preguntas contrafactuales. En el caso de las redes neuronales, las neuronas dopaminérgicas se les consideraba responsables de efectuar un método libre de modelos conocido como Aprendizaje por Diferencias Temporales ("Temporal Difference Learning"), pero la reciente evidencia parece sostener que sistemas de neuronas más grandes parecen generar un método basado en modelos (Wunderlich, Smittenaar y Dolan, 2012; Langdon, Sharpe, Schoenbaum, 2018). 

Por último, es notorio observar que Pearl tiene cierto rechazo hacia la aparente falta de explicabilidad de los modelos de Deep Learning. Si bien este tema es muy debatido, para algunas perspectivas sí es posible hablar de explicabilidad dentro del Deep Learning. Para Cichy y Kaiser (2019), las arquitecturas profundas de redes neuronales pueden considerarse como verdaderos modelos cognitivos, ya que tienen un mayor poder predictivo que otros modelos "más explicables" y además pueden ser útiles para sugerir hipótesis de cómo operan las redes biológicas. Por último, existen diversos métodos para interpretar los modelos de cajas negras de redes neuronales artificiales, tales como los describen Zhang _et al_ (2021).

## Conclusiones

La postura que ofrece Pearl en su artículo (2019) es generalmente negativa hacia el estado de arte del Aprendizaje Automático, aunque reconoce que pueden llegar a mostrar avances impresionantes. La tesis central de Pearl radica en que los avances en Aprendizaje Automático actuales no pueden alcanzar un nivel de inteligencia comparable al humano, ya que operan exclusivamente en datos. Sin embargo, esto no descarta a la posibilidad de mejorar dichos sistemas para lograr superar estas limitaciones iniciales. Pearl es muy claro sobre qué aspectos deben considerarse: las preguntas intervencionales y contrafactuales. Lo primero puede tratarse mediante la construcción de agentes que _intervengan_ en el ambiente. Sin embargo esto ya existe: el Aprendizaje por Refuerzo (no mencionado por Pearl) puede generar agentes que activamente estén interviniendo en su entorno e inclusive modificándolo para aprender del mismo. En cuanto a las preguntas contrafactuales, hemos dicho que los métodos de Aprendizaje por Refuerzo basados en modelos pueden tratar de construir representaciones del mundo que puedan tratar estos problemas. Esto no implica que las observaciones de Pearl están desacreditadas: este autor nos da la dirección que debe seguir la Inteligencia Artificial para aproximarse aún más a los niveles cognitivos humanos. Si esta tarea es _a priori_ posible o imposible deberá tomar otras consideraciones. 


## Referencias
- Chomsky, N. (2014). _Aspects of the Theory of Syntax_ (Vol. 11). MIT press.
- Cichy, R. M., & Kaiser, D. (2019). Deep neural networks as scientific models. _Trends in cognitive sciences_, _23_(4), 305-317.
- Duschinsky, R. (2012). Tabula rasa and human nature. _Philosophy_, _87_(4), 509-529.
- Langdon, A. J., Sharpe, M. J., Schoenbaum, G., & Niv, Y. (2018). Model-based predictions for dopamine. _Current opinion in neurobiology_, _49_, 1-7.
- Pearl, Judea. (2019). “[The Limitations of Opaque Learning Machines](https://ftp.cs.ucla.edu/pub/stat_ser/r489.pdf).” _Possible Minds: Twenty-Five Ways of Looking at AI_, 13–19.
- Shanahan, M. (2015). _The technological singularity_. MIT press.
- Wunderlich, K., Smittenaar, P., & Dolan, R. J. (2012). Dopamine enhances model-based over model-free choice behavior. _Neuron_, _75_(3), 418-424.
- Zhang, Y., Tiňo, P., Leonardis, A., & Tang, K. (2021). A survey on neural network interpretability. _IEEE Transactions on Emerging Topics in Computational Intelligence_.