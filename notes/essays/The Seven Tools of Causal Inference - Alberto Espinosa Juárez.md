# Ensayo de "The seven tools of causal inference, with reflections on machine learning"
Me parece interesante como ambos textos (este y [[The Limitations of Opaque Learning Machines]]) hablan sobre los contrafácticos, aunque fueron escritos por la misma persona, empiezo a creer que podría ser el punto clave de hacer una IA verdaderamente inteligente y que el camino podría ser el modelado causal debido a lo sustentado y razonable que parece.
Respecto a la jerarquía causal de la que habla [Judea Perl] parece ser viable; tener 3 niveles, yendo por complejidad, sin embargo, ¿Qué tan cerca estamos de ello? Por ejemplo, para el nivel uno, podría incluir aquellos experimentos donde se analiza en visión artificial, los pixéles que fueron seleccionados por la red neuronal para poder arrojar como respuesta un "algo", de allí se podría partir para empezar a trabajar en responder aquellas preguntas típicas de las que habla Judea, sin embargo, los niveles restantes (2 y 3) parecen estar muy lejos, pues, personalmente, me suenan a que un aprendizaje supervisado no bastaría ni podría obtener dichos niveles, por lo cual, la única opción, sería, nuevamente, el aprendizaje no supervisado.
Respecto al motor de inferencia SCM y por lo tanto, las 7 herramientas de las cuales habla y se centra el texto, también me es interesante, pues a simple vista no parece ser un modelo complejo, sin embargo, lo complejo viene en las suposiciones y datos que necestia el modelo (desde mi punto de vista) y menciona algo muy cierto y eso es que las herramientas siempre han ayudado a transformar la ciencia.

Respecto a las herramientas, la herramienta número uno habla sobre la transparencia para poder comprobar supuestos. La transparencia siempre ha sido un caso de interés en muchas áreas y esta no es la excepción, pues es la misma transparencia la que nos permite poder responder preguntas sobre lo que se está haciendo y con ello, comprobar dichos supuestos.
Respecto a esto, me gustaría agregar que aquí empieza un camino bastante complicado, pues hoy en día, ni siquiera tenemos transparencia en muchos de los modelos que se usan, causando sesgos o inclinaciones no deseadas. La herramienta número dos, títulada como do-cálculo y el control de la confusión, habla sobre aquellos sesgos que se llegan a presentar en las variables que contemplamos y es que tal como se menciona, el poder seleccionar un conjunto de covariables correctas para evitar esa confusión se ha vuelto algo manejable, viable por medio de algoritmos y para aquellos otros donde el críterio de la puerta trasera no se cumple, pues entra el do-calculus, que se basa en predecir las intervenciones siempre que sea factible. A partir de la herramienta número 3, llamada "la algortimización de contrafactuales", desde mi punto de vista es donde se pone interesante todo, porque se habla de los logros que ha tenido la humanidad respecto a formalizar el razonamiento contrafáctico y de cual será ese "valor de verdad". Respecto a las herramientas cuatro y cinco, donde se ve el mecanismo de transmisión y la adaptibilidad, validez y el sesgo de la muestra, debemos de entender, nuevamente, tal como lo mencionaba en el ensayo [[The Limitations of Opaque Learning Machines - Alberto Espinosa Juárez]], los modelos actuales funcionan bien hasta que dicho modelo sale de su zona de confort, es decir, hasta que se le presentan datos o un ambiente distinto al que fue diseñado... Lo más seguro es que deje de funcionar. Respecto a la herramienta 6 y 7, que van hacía la recuperación de datos y el descubrimiento causal, me agrada el que estas dos herramientas estén separadas y no operen juntas, esto porque el razonamiento causal es indispensable para el ser humano y el poderlo algortimizar puede llevar a una siguiente revolucion de [Inteligencia Artificial], sin embargo ¿Qué tan cerca estamos de ello?