# Ensayo sobre "The limitations Of Opaque Learning Machines"
[Judea Pearl] habla de un tema muy interesante que se puede resumir en una frase escrita en el mismo texto "Funciona bien y no sabemos por qué". El aprendizaje profundo ha avanzado mucho estos últimos años y año con año vemos revoluciones en esta área de la [Inteligencia Artificial]. Nos puede sorprender como [AlphaCode] hoy en día puede generar código de programación pero utilizando (o al menos intentando) utilizar el mejor algoritmo posible para su solución o como GPT3 aprendía a programar sin incluso ser entrenada para ello y a todo esto, hacernos la pregunta del ¿Por qué funciona tan bien? puede llevarnos hoy en día a un callejón sin salida aparente, aunque desde mi punto de vista, tal vez es que estamos comprendiendo mal las cosas.
Lo primero que debemos de comprender es que las redes neuronales artificiales y su inspiración biológico, el cerebro, no son iguales ni funcionan de forma cercana (a pesar de que las RNA están inspiradas en la neurona del cerebro humano). No existe forma de comparar el trabajo realizado por nuestro cerebro a las RNA, pues no existe todavía el poder computacional para poder simularlo, por ejemplo, de forma completa, mucho menos de crear una RNA con tal cantidad de neuronas e interconexiones como lo tiene nuestro cerebro. Otra cosa es que a pesar de haberse inspirado en las neuronas del cerebro, hoy en día sabemos que no es el único lugar donde las tenemos y parece estar un poco lejos esa misión de comprender de forma perfecta como es que ese pulso electroquimico que recibe la neurona es capaz de transformar dicho pulso en recuerdos, conocimiento, aprendizaje, movimientos y un lago etcétera. Por lo anterior, desde un punto de vista personal, es imposible todavía en pensar que una IA sea capaz de razonar, mucho menos emular de forma cercana al ser humano, nuevamente: No somos lo mismo.

Otro elemento interesante que realiza [Judea Pearl] es que lo que ha ayudado al ser humano a evolucionar a tal punto de poder construir herramientas por si mismo, es el hecho de tener una representación mental del entorno donde nos encontramos y resulta interesante para mí puesto que, como apasionado del aprendizaje profundo, este no lo tiene. Es necesario siempre brindar datos sobre en dónde se realizará la tarea para la que esa red neuronal está siendo diseñada, pues en caso de no encontrarlos, seguro el resultado será malo y pos si sola, no tiene oportunidad de crear una representación ¿Virutal? de su entorno. Tal vez por ello mencionan que el fúturo de la IA es el aprendizaje no supervisado y coincidiría en dejar que las inteligencias artificiales tomen rumbos y aprendan por su cuenta derivado de los datos no etiquetados que se le presenten, sin embargo, nuevamente no podríamos tener claro un "cómo lo está haciendo". 

Respecto al ámbito de la inferencia causal dentro de la IA, no puedo más que coincidir con el autor. Debemos tener en mente y estar preparados para cuando querramos realizar una IA que sea capaz de razonar "inteligentemente" y no pueda hacerlo, porque razonar estádisticamente es muy distinto a razonar derivado de posibles causas "si hacemos esto otro..." o "si fuese distinto...". Hace tiempo en una charla me dijeron algo que hoy en día sigo compartiendo y es que una IA no podrá ser parecido a un ser humano hasta que pueda entender el cómo y por qué somos capaces de atravesar un semáforo en rojo, olvidar las llaves en el sofa o cambiar de camino únicamente porque se nos antojó... y claro, que pueda replicarlo. 